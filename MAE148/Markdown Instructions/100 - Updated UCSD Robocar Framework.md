<!-- Copy and paste the converted output. -->

<!-----

You have some errors, warnings, or alerts. If you are using reckless mode, turn it off to see inline alerts.
* ERRORs: 19
* WARNINGs: 0
* ALERTS: 48

Conversion time: 8.243 seconds.


Using this Markdown file:

1. Paste this output into your source file.
2. See the notes and action items below regarding this conversion run.
3. Check the rendered output (headings, lists, code blocks, tables) for proper
   formatting and use a linkchecker before you publish this page.

Conversion notes:

* Docs to Markdown version 1.0β36
* Tue May 21 2024 12:19:44 GMT-0700 (PDT)
* Source doc: Copy of 13May24 - Copy of 100-UCSD Robocar Framework

ERROR:
undefined internal link to this URL: "#heading=h.3ucils4zejvp".link text: here
?Did you generate a TOC with blue links?

* Tables are currently converted to HTML tables.

ERROR:
undefined internal link to this URL: "#heading=h.e4kj9qogzk71".link text: Make sure that an X11 forwarding session was established when doing an ssh connection into the jetson 
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.5y4lnb6i5xhk".link text: creating a new container using the provided function in the ~/.bashrc
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.8o8rk32jxew1".link text: virtual machine
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.trei9q51sjv8".link text: docker commands sections
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.dydr21q8ok61".link text: Updating all packaging in the ucsd_robocar framework from gitlab:
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.kh752plifc0s".link text: Source Noetic and ALL ROS packages and start roscore
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.kh752plifc0s".link text: Source Noetic and ALL ROS packages
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.kh752plifc0s".link text: Source Noetic and ALL ROS packages and put user in ros1_ws:
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.hmc07r1l07ue".link text: Source foxy and ALL ROS2 packages:
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.hmc07r1l07ue".link text: Source foxy and ALL ROS2 packages and put user in ros2_ws:
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.hmc07r1l07ue".link text: Build all packages in ucsd_robocar:
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.4r55s7fis99g".link text: Source ROS bridge:
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.tlvigrv5i0a6".link text: basics package
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.hklk77k10dvo".link text: actuator calibration
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.hmc07r1l07ue".link text: Source ROS2
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.jd5irxt7cia5".link text: calibration
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.b1yz3c58rzq6".link text: where a lidar and actuator type has been selected
?Did you generate a TOC with blue links?


ERROR:
undefined internal link to this URL: "#heading=h.vneeh6hyq5n5".link text: node_config
?Did you generate a TOC with blue links?

* This document has images: check for >>>>>  gd2md-html alert:  inline image link in generated source and store images to your server. NOTE: Images in exported zip file from Google Docs may not appear in  the same order as they do in your doc. Please check the images!

----->


<p style="color: red; font-weight: bold">>>>>>  gd2md-html alert:  ERRORs: 19; WARNINGs: 0; ALERTS: 48.</p>
<ul style="color: red; font-weight: bold"><li>See top comment block for details on ERRORs and WARNINGs. <li>In the converted Markdown or HTML, search for inline alerts that start with >>>>>  gd2md-html alert:  for specific instances that need correction.</ul>

<p style="color: red; font-weight: bold">Links to alert messages:</p><a href="#gdcalert1">alert1</a>
<a href="#gdcalert2">alert2</a>
<a href="#gdcalert3">alert3</a>
<a href="#gdcalert4">alert4</a>
<a href="#gdcalert5">alert5</a>
<a href="#gdcalert6">alert6</a>
<a href="#gdcalert7">alert7</a>
<a href="#gdcalert8">alert8</a>
<a href="#gdcalert9">alert9</a>
<a href="#gdcalert10">alert10</a>
<a href="#gdcalert11">alert11</a>
<a href="#gdcalert12">alert12</a>
<a href="#gdcalert13">alert13</a>
<a href="#gdcalert14">alert14</a>
<a href="#gdcalert15">alert15</a>
<a href="#gdcalert16">alert16</a>
<a href="#gdcalert17">alert17</a>
<a href="#gdcalert18">alert18</a>
<a href="#gdcalert19">alert19</a>
<a href="#gdcalert20">alert20</a>
<a href="#gdcalert21">alert21</a>
<a href="#gdcalert22">alert22</a>
<a href="#gdcalert23">alert23</a>
<a href="#gdcalert24">alert24</a>
<a href="#gdcalert25">alert25</a>
<a href="#gdcalert26">alert26</a>
<a href="#gdcalert27">alert27</a>
<a href="#gdcalert28">alert28</a>
<a href="#gdcalert29">alert29</a>
<a href="#gdcalert30">alert30</a>
<a href="#gdcalert31">alert31</a>
<a href="#gdcalert32">alert32</a>
<a href="#gdcalert33">alert33</a>
<a href="#gdcalert34">alert34</a>
<a href="#gdcalert35">alert35</a>
<a href="#gdcalert36">alert36</a>
<a href="#gdcalert37">alert37</a>
<a href="#gdcalert38">alert38</a>
<a href="#gdcalert39">alert39</a>
<a href="#gdcalert40">alert40</a>
<a href="#gdcalert41">alert41</a>
<a href="#gdcalert42">alert42</a>
<a href="#gdcalert43">alert43</a>
<a href="#gdcalert44">alert44</a>
<a href="#gdcalert45">alert45</a>
<a href="#gdcalert46">alert46</a>
<a href="#gdcalert47">alert47</a>
<a href="#gdcalert48">alert48</a>

<p style="color: red; font-weight: bold">>>>>> PLEASE check and correct alert issues and delete this message and the inline alerts.<hr></p>



## **UCSD Robocar Framework**

Version V1.3

Last updated: 08/26/2022

Prepared by

Dominic Nightingale

Department of Mechanical and Aerospace Engineering

University of California, San Diego

9500 Gilman Dr, La Jolla, CA 92093

	

<p id="gdcalert1" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image1.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert2">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image1.png "image_tooltip")


	

<p id="gdcalert2" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image2.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert3">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image2.png "image_tooltip")


	

<p id="gdcalert3" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image3.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert4">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image3.png "image_tooltip")


		


## 


## Table of Contents


[TOC]



## 


## 1. Introduction 

The UCSD Robocar framework is primarily maintained and developed by Dominic Nightingale right here at UC San Diego.

UCSD Robocar uses ROS and ROS2 for controlling our scaled robot cars which can vary from traditional programming or machine learning to achieve an objective. The framework works with a vast selection of sensors and actuation methods in our inventory making it a robust framework to use across various platforms. Has been tested on 1/16, 1/10, 1/5 scaled robot cars and soon our go-karts.


### 1.1 About

This framework was originally developed as one of Dominic’s senior capstone projects as an undergraduate and has been under constant development throughout his graduate program. The framework provides the ability to easily control a car-like robot as well as performing autonomous tasks. It is currently being used to support his thesis in learning-model predictive control (LMPC).

The framework is also being used to teach undergraduates the fundamentals of using gitlab, docker, python, openCV and ROS. The students are given the task to use the framework with their robots to perform autonomous laps on a track by first going through a calibration process that's embedded into the framework. The students then have to come up with their own final projects for the class that can be supported by the framework, which can vary from car following, SLAM applications, path planning, city driving behaviors, Human-machine-interfacing and so much more.


### 1.2 What's Being Used


#### 1.2.1 Embedded Computers

There are 3 main computers that have been used to develop and test this framework which belong to the NVIDIA Jetson family. 



* Jetson Nano
* Jetson Xavier Nx
* Jetson AGX Xavier


#### 1.2.2 Ubuntu

The host OS on all the Jetson computers use Ubuntu18 which is flashed through NVIDIA's Jetpack image. However, the docker image uses Ubuntu20 in order to use ROS2 without worrying about package installation issues


#### 1.2.3 Gitlab

This is where all the code for the entire framework is managed and developed. Gitlab provides a service similar to google drive but for programs! It's especially convenient in terms of deploying code into embedded computers.


#### 1.2.4 Docker

This tool is being used to expedite the setup process on the computers. To get the docker image working, the Jetson just needs to be flashed with the Jetpack 4.6 image provided by NVIDIA and then simply pull the UCSD Robocar docker image from docker hub onto the Jetson. This allows for plug-n-play capabilities as long as all the hardware is connected to the Jetson properly.


#### 1.2.5 ROS

The framework allows for both ROS-Noetic and ROS2-Foxy to work together through the ROS bridge or independently depending on the application.


### 1.3 Recommendations


#### 1.3.1 VS Code IDE

Microsoft Visual Studio IDE is an excellent development tool for coding especially because of all the free plug-ins that can be added.

Plug-ins recommended:



* Python
* Docker
* Remote - SSH


#### 1.3.2 Virtual Machines

If having software related issues, a virtual machine can possibly solve the issues and also provide a linux based interface to use with the jetson which is usually much smoother than with windows or mac.

Below are some links to install Virtual machine software and a virtual machine image that runs Ubuntu20.04, has VS code (with all plug-ins mentioned above), docker and the UCSDrobocar docker image installed already.

[VMware Software](https://www.vmware.com/products/workstation-player.html)

[UCSD Robocar VM image for VMware](https://drive.google.com/file/d/1ltKrZBdA2ZTFRjKj5E08WWbZN5PNIPJL/view?usp=sharing)

Hostname: ucsdrobocar-vm

Username: robocar

Password: ucsdrobocar


## 2. UCSD Robocar Framework Breakdown

Below are the supporting packages to the framework. The Nav package operates as the "brain" because it is the only package that communicates to all the other packages which are all independent from one another.

​

Why so many packages? In practice, developing stand-alone or independent functionalities makes the package more robust in terms of deployability. Also as the robot becomes more sophisticated, the number of packages it will have access to would naturally increase allowing it to achieve many different types of tasks depending on the application of interest.

So the idea is to develop a package that could in general be used on any car-like robot as well as being able to choose what packages your robot really needs without having to use the entire framework. 

​

For example, lets say another company developed their own similar sensor, actuator and nav packages but they have not researched into lane detection. Instead of using the entire UCSD Robocar framework, they could easily just deploy the lane detection package and have some interpreter in their framework read the messages from the lane detection package to suit their needs.

Link to official git repo (**ROS1**): [ucsd_robocar_hub1](https://gitlab.com/ucsd_robocar/ucsd_robocar_hub1)

Link to official git repo (**ROS2**): [ucsd_robocar_hub2](https://gitlab.com/ucsd_robocar2/ucsd_robocar_hub2)

NOTE: Both hub1 and hub2 are _<span style="text-decoration:underline;">metapackages</span>_. For specific details about any individual package, click on any of the packages in either hub to be taken to that packages' main repository.


### 2.1 Packages

**Each UCSD ROS package has a README.md that explains in detail what config, nodes, launch files it has as well as topic/message information. So if you are confused about a particular thing, ask yourself, **

**“What is the problem I am having?” ,“What package is most likely the root of the concern?” Then go see the readme for that package and check anything relevant or even the troubleshooting section.**

In the package sections below are the links to the official README.md docs for each package for both ROS1 and ROS2. So any package with a 1 in it is for ROS-NOETIC and any package with a 2 is for ROS2-FOXY.


#### 2.1.1 Nav 

The navigation package (nav_pkg) is the "brain" of the UCSD Robocar framework because it keeps all the launch files in its package to launch any node/launch file from the other packages used in the framework. This makes using the framework easier because you only really have to remember the name of the nav_pkg and what launch file you want to use rather than having to remember all the other package names and their own unique launch files.

[NAV2 README.md](https://gitlab.com/ucsd_robocar2/ucsd_robocar_nav2_pkg/-/blob/master/README.md)

[NAV1 README.md](https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg/-/blob/master/README.md)


#### 2.1.2 Lane Detection

The lane detection package is one method of navigating by identifying and tracking road markers. The basic principle behind this package is to detect road markers using openCV and then compute whats called the “cross-track-error” which is the difference between the center axis of the car and the centroid (center of “mass”) of the road mark which is then fed into a PID controller for tracking.  

[Lane Detection2  README.md](https://gitlab.com/ucsd_robocar2/ucsd_robocar_lane_detection2_pkg/-/blob/master/README.md)

[Lane Detection1 README.md](https://gitlab.com/ucsd_robocar/ucsd_robocar_lane_detection1_pkg/-/blob/master/README.md)


#### 2.1.3 Sensor

The sensor package contains all the required nodes/launch files needed to use the sensors that are equipped to the car.

[Sensor2 README.md](https://gitlab.com/ucsd_robocar2/ucsd_robocar_sensor2_pkg/-/blob/master/README.md)

[Sensor1 README.md](https://gitlab.com/ucsd_robocar/ucsd_robocar_sensor1_pkg)


#### 2.1.4 Actuator

The actuator package contains all the required nodes/launch files needed to use the actuators that are equipped to the car.

[Actuator2 README.md](https://gitlab.com/ucsd_robocar2/ucsd_robocar_actuator2_pkg/-/blob/master/README.md)

[Actuator1 README.md](https://gitlab.com/ucsd_robocar/ucsd_robocar_actuator1_pkg/-/blob/master/README.md)


#### 2.1.5 Control (coming soon)

The control package contains all the required nodes/launch files needed to control the car in various methods such as PID, LQR, LQG and MPC

.


#### 2.1.6 Path (coming soon)

The path package contains all the required nodes/launch files needed to create trajectories for the car to follow in a pre-built map as well as in simulations 


#### 2.1.7 Basics

The path package contains all the required nodes/launch files needed to subscribe/publish to the sensor/actuator messages within the framework for fast algorithm prototyping

[Basics2 README.md](https://gitlab.com/ucsd_robocar2/ucsd_robocar_basics2_pkg/-/blob/master/README.md)


### 2.2 Updating All Packages

A utility function was added to the ~/.bashrc script that will automatically update all the packages in the framework and then rebuild and source it so it will be ready to start using ROS2!  

From the terminal


```
upd_ucsd_robocar
```



### 


### 2.2 Launch Files

The launch file diagrams below show the very general approach of how the packages communicate with one another. With ROS, it just comes down to a combination of starting launch files and sending messages (through topics) to nodes. For specific details about messages types, topics, services and launch files used, please go to the readme for the specific package of interest!

The nav_pkg is at the base of each of the diagrams and rooting from it are the launch files it calls that will launch other nodes/launch files from all the other packages in the framework.

In ROS2, a _<span style="text-decoration:underline;">dynamically</span>_ built launch file (at run-time) is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a “0” or a “1” next to the list of nodes/launch files. To select the nodes that you want to use, put a “1” next to it otherwise put a “0” which means it will not activate. In the figures below, instead of including the entire ros2 launch command, you will only see the names of the launch files that need to be turned on in the node config file explained more in detail 

<p id="gdcalert4" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "here"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert5">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[here](#heading=h.3ucils4zejvp)


<table>
  <tr>
   <td>

<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image4.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image4.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>ROS-NOETIC: <code>roslaunch ucsd_robocar_nav1_pkg sensor_visualization.launch</code>
<p>
ROS2-FOXY: <code>all_components.launch.py, sensor_visualization.launch.py</code>
   </td>
  </tr>
  <tr>
   <td>

<p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image5.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>ROS-NOETIC: <code>roslaunch ucsd_robocar_nav1_pkg teleop_joy_vesc.launch</code>
<p>
ROS2-FOXY:<code>all_components.launch.py, teleop_joy_vesc_launch.launch.py</code>
   </td>
  </tr>
  <tr>
   <td>

<p id="gdcalert7" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert8">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image6.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>ROS-NOETIC: <code>roslaunch ucsd_robocar_nav1_pkg camera_nav_calibration_launch.launch</code>
<p>
ROS2-FOXY: <code>all_components.launch.py, camera_nav_calibration.launch.py</code>
   </td>
  </tr>
  <tr>
   <td>

<p id="gdcalert8" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert9">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image7.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>ROS-NOETIC: <code>roslaunch ucsd_robocar_nav1_pkg camera_nav_launch.launch</code>
<p>
ROS2-FOXY: <code>all_components.launch.py, camera_nav.launch.py</code>
   </td>
  </tr>
  <tr>
   <td>

<p id="gdcalert9" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image8.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert10">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image8.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>ROS-NOETIC: <code>roslaunch ucsd_robocar_nav1_pkg ros_racer_mapping_launch.launch</code>
   </td>
  </tr>
  <tr>
   <td>

<p id="gdcalert10" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image9.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert11">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image9.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>ROS-NOETIC: <code>roslaunch ucsd_robocar_nav1_pkg ros_racer_nav_launch.launch</code>
   </td>
  </tr>
</table>



## 


## 3. Developer Tools


### 3.1 ROS Guidebooks

Links provided below are guides for ROS and ROS2 which include many examples, terminal commands and general concept explanations of the various features in ROS and ROS2



* [UCSD ROS Guidebook](https://docs.google.com/document/d/1u7XS7B-Rl_emK3kVKEfc0MxHtwXGYHf5HfLlnX8Ydiw/edit)
* [UCSD ROS2 Guidebook](https://docs.google.com/document/d/1DJgVLnu_vN-IXKD3QrQVF3W-JC6RiQPVugHeFAioB58/edit?usp=sharing)


### 3.2 Gitlab

Since the framework uses a meta package (a package that contains multiple packages) we refer to _individual packages_ as _submodules_.


#### 3.2.1 Adding new submodules:



1. `git submodule add &lt;remote_url>`
2. `git commit -m "message"`
3. `git push`


#### 3.2.2 Updating **local **submodules with **remote **submodules:



1. If local changes have been made, the update command will fail unless you add, commit and push (shown in 3.2.3) or stash (`git stash`) them, which will temporarily discard any local changes
2. `git submodule update --remote --merge` <span style="text-decoration:underline;">Pay attention to the output of this command, to make sure it did not fail or Abort...</span>


#### 3.2.3 Updating **remote **submodules with **local **submodules:



1. `git add .`
2. `git commit -m "message"`
3. `git push` <span style="text-decoration:underline;">Pay attention to the output of this command, to make sure it did not fail or Abort...</span>


#### 3.2.4 Removing submodules:



1. `git submodule deinit &lt;submodule>`
2. `git rm &lt;submodule>`


#### 


#### 3.2.5 Adding an existing package to git

From the web browser, [create empty repo on gitlab](https://docs.gitlab.com/ee/user/project/working_with_projects.html#create-a-blank-project) 

Now from the Jetson, start by creating a new ROS2 package


```
ros2 pkg create --build-type ament_python pkg_name --dependencies rclpy
build_ros2
```


Now proceed with merging the new package with the framework


```
git init
git remote add origin <remote url from step 1>
git add .
git commit -m "message"
git push --set-upstream origin master
```



### 


### 3.3 Docker

Below is a go-to list of docker commands that can be used with the framework.

Some new lingo: 

Container name: **NAMES**

Image name: **REPOSITORY**

Image tag ID (comparable to branches in git): **TAG**


#### 3.3.1 Pulling/running



* pulling image from docker hub: `docker pull REPOSITORY:TAG`
* starting a stopped container: `docker start NAMES`
* stopping a container: `docker stop NAMES `
* Using Multiple Terminals for a Single Docker Container: `docker exec -it NAMES bash`
* build docker image and give it a new name and tag  `docker build -t REPOSITORY:TAG .   `


#### 3.3.2 Updating/creating/sharing



* save changes made while in container to original image (change tag to create a new image):  \
`docker commit name_of_container REPOSITORY:TAG`
* create a new image from a container: `docker tag NAMES REPOSITORY:TAG`
* pushing image to dockerhub: `docker push REPOSITORY:TAG`
* Share files between host and docker container:  
    * From **host **to docker container: `docker cp foo.txt container_id:/foo.txt`
    * From **docker container** to host: `docker cp container_id:/foo.txt foo.txt`


#### 3.3.3 Listing



* list all images: `docker images`
* list all running containers: `docker ps`
* list all containers (including stopped): `docker ps  -a`


#### 3.3.4 Deleting



* delete specific container: `docker rm NAMES`
* delete specific image: `docker rmi REPOSITORY:TAG`
* delete ALL containers: `docker rm -f $(docker ps -a -q)`
* delete ALL images: `docker rmi -f $(docker images -q)`


## 


## 4. Accessing Docker Images

Currently there are two **DIFFERENT **docker images that are being supported by UCSD. One image was built for arm architecture computers (Jetson family) and the other was built for X86 architecture computers (most laptops and desktops). Apple M1 support will be coming soon.

**Question:** Why two images? 

**Answer:** The X86 image was built to provide an environment for the developer to test new algorithms, packages, sensors (Yes, you can plug sensors into your computer just like the Jetson for testing) etc in a simulated environment without having to use a physical robot. Using the physical robot for first-time testing can lead to damaging the robot or something/someone in the environment due to an unforeseen behavior from the robot. We must practice safe autonomy if we ever hope to see our new ideas become a part of the industry! This leads to the ARM image, which was built to be used on the physical robot when ready to perform physical testing.

**Question:** The display wont open when in the container, how to make it work? (ie. images won't port through)

**Answer: **There could be several reasons why the display is not working but below are the most common solutions that can be tried



* 

<p id="gdcalert11" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Make sure that an X11 forwarding session was established when doing an ssh connection into the jetson "). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert12">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Make sure that an X11 forwarding session was established when doing an ssh connection into the jetson ](#heading=h.e4kj9qogzk71)
* If that still doesn't work, then the container could have a broken connection with the display so the only other thing to try is 

<p id="gdcalert12" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "creating a new container using the provided function in the ~/.bashrc"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert13">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[creating a new container using the provided function in the ~/.bashrc](#heading=h.5y4lnb6i5xhk)

**NOTE:** Docker is pre-installed on the Jetson computers so no need to install it, but in order to use the X86 image, you must install docker on your computer (for linux, windows or mac).


### 4.1 UCSD Robocar Image

Link to image on Docker Hub: [docker image](https://hub.docker.com/r/djnighti/ucsd_robocar)

Computer architecture: ARM (Jetson)

Pulling the image from the terminal:


```
docker pull djnighti/ucsd_robocar:latest
```


Computer architecture: X86 (Most laptops and desktops)

Pulling the image from the terminal:


```
docker pull djnighti/ucsd_robocar:x86
```



### 


### 4.2 Docker Setup

The exact "recipe" to build this image can be found [here](https://gitlab.com/ucsd_robocar2/ucsd_robocar_hub2/-/blob/master/docker_setup/docker_files/Dockerfile)

**<span style="text-decoration:underline;">Note: If using the 

<p id="gdcalert13" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "virtual machine"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert14">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[virtual machine](#heading=h.8o8rk32jxew1), all this is already completed for you!</span>**


#### 4.2.1 Enable X_11 Port Forwarding



1. On your **HOST **machine **<span style="text-decoration:underline;">(not the Jetson)</span>** enter these commands (Will have to enter every time)

    ```
    xhost +


##     ssh -X jetson@ip_address
    ```


2. Now on the **Jetson**, run the following commands to obtain sudo access for docker commands (only needs to be ran once)

    ```


##     sudo usermod -aG docker ${USER}


##     su ${USER}
    ```


3. Now check that if X_11 forwarding is working 

    ```


##     xeyes

    ```


If some googly eyes pop up, X_11 is ready to go. IF X_11 PORT FORWARDING IS NOT SETUP, follow steps [here](https://gitlab.com/djnighti/ucsd_robo_car_simple_ros/-/blob/master/x11_forwarding_steps.txt) to get it set up. Then come back here to continue the steps below.


#### 4.2.2 Update Docker Daemon 



1. Then modify daemon.json file (just delete previous version then create new one)

    ```


##     sudo rm /etc/docker/daemon.json


##     sudo nano /etc/docker/daemon.json
    ```


2. copy and paste the following into that file:

    ```
{
    "runtimes": {
        "nvidia": {
            "path": "nvidia-container-runtime",
            "runtimeArgs": []
        }
    },
    "default-runtime": "nvidia"
}
```


3. save and quit then reboot jetson

    ```
    sudo reboot now

    ```



#### 4.2.3 Running A Container



1. SSH back into the Jetson with the -X flag which enables X_11 Forwarding

    ```


##     ssh -X jetson@ip_address
    ```


2. Create a new function in the ~/.bashrc file with command line arguments to easily run a container

    ```
    gedit ~/.bashrc
    ```


3. Copy and paste the following into the very bottom of the file

    ```
robocar_docker ()
{
    docker run \
    --name ${1}\
    -it \
    --privileged \
    --net=host \
    -e DISPLAY=$DISPLAY \
    -v /dev/bus/usb:/dev/bus/usb \
    --device-cgroup-rule='c 189:* rmw' \
    --device /dev/video0 \
    --volume="$HOME/.Xauthority:/root/.Xauthority:rw" \
    djnighti/ucsd_robocar:${2:-devel}
}
```



    **Notice the two arguments we have made:**


    **${1}:** This will be the name of the container, ex. Name_this_container


    **${2:devel}:** This is the tag id of the image you want to launch a container from. If nothing is specified when calling at the command line (example shown below), the “`devel`” tag will be run. 


    **<span style="text-decoration:underline;">Don't modify the function, the arguments are intentional and not meant to be hard coded.</span>**

4. Source the ~/.bashrc script so the current terminal can see the new function we just added

    ```
    source ~/.bashrc
    ```


5. Run the following command to enter the docker container

    ```
    robocar_docker test_container
    ```


6. To access the **<span style="text-decoration:underline;">same</span> **docker container from another terminal (do this for as many terminals you want)

    ```
    docker exec -it test_container bash 

    ```


At this point the docker setup is complete but don't forget to refer to the useful 

<p id="gdcalert14" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "docker commands sections"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert15">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[docker commands sections](#heading=h.trei9q51sjv8) which includes deleting, creating and updating images locally and remotely!




### 4.3 Workspaces in Docker Container


#### 4.3.1 ros1_ws

ROS version: **ROS-NOETIC**

This workspace contains source compiled packages from **[ucsd_robocar_hub1](https://gitlab.com/ucsd_robocar/ucsd_robocar_hub1)**


#### 4.3.2 ros2_ws

ROS version: **ROS2-FOXY**

This workspace contains source compiled packages from **[ucsd_robocar_hub2](https://gitlab.com/ucsd_robocar2/ucsd_robocar_hub2)**


#### 4.3.3 sensor2_ws

ROS version: **ROS2-FOXY**

This workspace contains source compiled packages for various sensors in our inventory. 


### 4.4 ROS BRIDGE 

The ros1_bridge package is used to enable the communication between nodes in ROS1 (ros1_ws) and ROS2 (ros2_ws). Reading material on how to use it can be found [here](https://industrial-training-master.readthedocs.io/en/melodic/_source/session7/ROS1-ROS2-bridge.html#run-the-ros1-bridge) and a video of it being used can be found [here](https://www.theconstructsim.com/how-to-communicate-between-ros1-ros2-with-ros1_bridge)

**REMEMBER:**

**<span style="text-decoration:underline;">This image has both ROS1 and ROS2 which results in having to source them individually and every new terminal. This also means that the metapackages ucsd_robocar_hub1 and ucsd_robocar_hub2 must be sourced!</span>**

Jetpack info for Jetson

**REQUIREMENT**: JetPack 4.6 (L4T R32.6.1)

check to make sure: `sudo apt-cache show nvidia-jetpack`


### 4.5 Utility functions in ~/.bashrc 



* 

<p id="gdcalert15" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Updating all packaging in the ucsd_robocar framework from gitlab:"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert16">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Updating all packaging in the ucsd_robocar framework from gitlab:](#heading=h.dydr21q8ok61) `upd_ucsd_robocar`
* 

<p id="gdcalert16" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source Noetic and ALL ROS packages and start roscore"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert17">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source Noetic and ALL ROS packages and start roscore](#heading=h.kh752plifc0s): `source_ros1_init`
* 

<p id="gdcalert17" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source Noetic and ALL ROS packages"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert18">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source Noetic and ALL ROS packages](#heading=h.kh752plifc0s): `source_ros1_pkg`
* 

<p id="gdcalert18" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source Noetic and ALL ROS packages and put user in ros1_ws:"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert19">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source Noetic and ALL ROS packages and put user in ros1_ws:](#heading=h.kh752plifc0s) `source_ros1`
* 

<p id="gdcalert19" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source foxy and ALL ROS2 packages:"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert20">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source foxy and ALL ROS2 packages:](#heading=h.hmc07r1l07ue) `source_ros2_pkg`
* 

<p id="gdcalert20" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source foxy and ALL ROS2 packages and put user in ros2_ws:"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert21">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source foxy and ALL ROS2 packages and put user in ros2_ws:](#heading=h.hmc07r1l07ue) `source_ros2`
* 

<p id="gdcalert21" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Build all packages in ucsd_robocar:"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert22">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Build all packages in ucsd_robocar:](#heading=h.hmc07r1l07ue) `build_ros2`
* 

<p id="gdcalert22" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source ROS bridge:"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert23">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source ROS bridge:](#heading=h.4r55s7fis99g) `source_ros_bridge`


## 5. Source ROS Version


### 5.1 Source ROS1 

We need to source ROS Noetic, ros1_ws and activate roscore, below is an alias command that will do all of that automatically. _<span style="text-decoration:underline;">This command only needs to be run one time in any docker container</span>_. As you open new terminals in the same Docker container, another alias was made to source ROS Noetic and the ros1_ws as well as placing you in the ros1_ws. _<span style="text-decoration:underline;">This command needs to be run in every new terminal you want to use ROS1 in.</span>_

From the terminal (this terminal will always need to be running so don't close it!)


```
source_ros1_init
```


From another terminal


```
source_ros1
```



### 5.2 Source ROS2

We need to source ROS Foxy and the ros2_ws, below is an alias command that will do that automatically. The alias will also place you in the ros2_ws. _<span style="text-decoration:underline;">This command needs to be run in every new terminal you want to use ROS2 in.</span>_ Another alias was made to rebuild the package if any changes were made to the source code. It will put you in the ros2_ws, then perform a colcon build and then source install/setup.bash to reflect the changes made. 

From the terminal


```
source_ros2
```


From the terminal (This is only needs to be ran in 1 terminal, the changes will be reflected everywhere)


```
build_ros2
```



### 5.3 Source ROS Bridge

We need to source ROS Noetic, ROS Foxy and the ros2_ws, below is an alias command that will do that and launch ros bridge automatically. _<span style="text-decoration:underline;">This command only needs to be run once and will occupy a terminal throughout its existence.</span>_ This alias does a dynamic bridge between ALL topics in Noetic and Foxy. 

From the terminal


```
source_ros_bridge
```



## 6. Hardware Configuration

Not all robots have the same hardware especially when it comes to their sensors and motors and motor controllers. This quick section shows how to select the hardware that is on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.


### 6.1 ROS1 

In ROS1, the hardware configuration is done by either modifying the launch files or at the command prompt as an argument to the launch command. There are 3 pre-made car configurations that can be launched at any time. If using a launch file from the nav_pkg, an example is given below for how to modify the launch file. All the launch files for the hardware can be found in the launch directory in the ucsd_robocar_nav1_pkg.



1. dsc_car_launch: sic lidar, intel camera (any model works), VESC
2. mae_148_car_launch: ld06 lidar, webcam, adafruit
3. custom_car_launch: ld06 lidar, webcam, VESC  (pick any nodes needed to launch all sensors/actuators on the car, by modifying the “custom_car_launch.launch” file)

**NOTE**: The custom car option is meant to be modified as needed for other types of configurations.

**Modify “load_car_launch.launch”  launch file with the car config for your robot. This is the line you need to modify, the 3 options are listed above. &lt;arg name="car_type" value="custom_car_launch " />**

From the terminal


```
source_ros1
gedit src/ucsd_robocar_hub1/ucsd_robocar_nav1_pkg/launch/load_car_launch.launch
```



### 6.2 ROS2

In ROS2, the hardware configuration is as simple as flipping a switch. Since the launch files in ROS2 are now in python, we can dynamically build launch files! This means no more need to have several different “car configs” that may have different hardware on them and instead have a single launch file that is capable of launching any component you need by changing a single number (that number is explained below)! There is only one file to modify and all that needs to be changed is either putting a “0” or a “1” next to the list of hardware in the file. To select the hardware that your robot has and that you want to use, put a “1” next to it otherwise put a “0” which means it will not activate.

**Modify and save the car config with the sensors and actuators on your robot <span style="text-decoration:underline;">and then recompile</span>.**

From the terminal


```
source_ros2
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
build_ros2
```



## 7. Node Configuration

This quick section shows how to select the nodes/launch files that are on your robot. There are differences between ROS1 and ROS2 on how this configuration works so please read accordingly. This configuration is only necessary for the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image.


### 7.1 ROS1 

In ROS1, the launch files for the various capabilities of the robot are written and called individually and can be found in the launch directory in the ucsd_robocar_nav1_pkg.


### 7.2 ROS2

Similar to the hardware configuration in ROS2, a dynamically built launch file is used to launch all the different nodes/launch files for various purposes such as data collection, navigation algorithms and controllers. This new way of creating launch files has now been simplified by just adding an entry to a yaml file of where the launch file is and a separate yaml file to indicate to use that launch file or not. There is only one file to modify and all that needs to be changed is either putting a “0” or a “1” next to the list of nodes/launch files. To select the nodes that you want to use, put a “1” next to it otherwise put a “0” which means it will not activate.

**Modify and save the node config to launch the algorithm(s) of your choice <span style="text-decoration:underline;">and then recompile</span>.**

From the terminal


```
source_ros2
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
build_ros2
```



## 8. Sensor Visualization

**After selecting the hardware that's equipped on the robot, let's visually verify that the sensors are working. The current config file that is launched will display laser scan and image data. If you have more sensors you want to visualize, feel free to add them through rviz.**


### 8.1 ROS1 

**Here is the list of available launch files for all the sensors in the [sensor1_pkg](https://gitlab.com/ucsd_robocar/ucsd_robocar_sensor1_pkg/-/tree/master/launch)**

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**

From terminal


```
source_ros1
roslaunch ucsd_robocar_nav1_pkg sensor_visualization.launch
```



### 8.2 ROS2

**Here is the list of available launch files for all the sensors in the [sensor2_pkg](https://gitlab.com/ucsd_robocar2/ucsd_robocar_sensor2_pkg/-/tree/master/launch)**

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**

From the terminal


```
source_ros2
```


Modify the hardware config file to turn on the **sensors **you have plugged in and want to visualize.


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate **all_components **and **sensor_visualization **launch files


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Then rebuild and launch 


```
build_ros2
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```


**NOTE: If image data does not show up automatically, un-check and check its box in the display panel in rviz.**

[Here is an example from intel showing the point cloud with any of their cameras in RVIZ!](https://github.com/IntelRealSense/realsense-ros/tree/ros2#point-cloud)

From the terminal


```
source_ros2
ros2 launch realsense2_camera demo_pointcloud_launch.py
```



## 9. Manual Control of Robot with Joystick

This feature is only supported in the UCSD Robocar Image and NOT UCSD Robocar Simple ROS Image

If using Adafruit and not VESC, anywhere below that says `vesc` you can replace with `adafruit`

A deadman switch is also enabled which means you _<span style="text-decoration:underline;">must </span>_be pressing the button (LB on logitech) down in order for you to send commands to your robots motors.

The joysticks on the controller are what control the robot to move forwards/backwards and turn.

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**


### 9.1 ROS1 

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**

From the terminal


```
source_ros1
roslaunch ucsd_robocar_nav1_pkg teleop_joy_vesc.launch
```



### 9.2 ROS2

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**

From the terminal


```
source_ros2
```


Modify the hardware config file to turn on the **vesc_with_odom**


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate **all_components **and **f1tenth_vesc_joy_launch **launch files


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Then rebuild and launch 


```
build_ros2
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```



## 10. Integrating New Packages/Code into the Framework

Integrating a new package can be done many ways so do not take this approach as the best or only method but simply a method for integration. The example below will be in ROS2 but the general procedure is the same in ROS1.

 


### 10.1 Integrating a ROS Package



1. While in the docker container source ros2 and move in to the src directory of the ros2_ws


```
source_ros2
cd src/

```



2. Now lets create a new node by [using an example node from the ros2 guidebook](https://docs.google.com/document/d/1DJgVLnu_vN-IXKD3QrQVF3W-JC6RiQPVugHeFAioB58/edit#heading=h.z8pzj9x72ptj) which gives all the code for the node, setup.py and launch files as well as step-by-step terminal commands to create everything including the package itself. 
    1. Package name: **counter_package**
    2. Node name: **counter_publisher.py**
    3. Launch file name: **counter_package_launch_file.launch.py**
3. After completing step 2, notice the **“counter_package”** package in the same directory as **“ucsd_robocar_hub2” **package


```
ls src/

```



4. Adding your package to the nav2 node configuration and node package location lists. To do this, all we need is the name of the package that we want to integrate and the name of the launch file we want to use from that package. In the example node above, the package name is **“counter_package”** and its launch file is called **“counter_package_launch_file.launch.py”**. Lets add them to “node_pkg_locations_ucsd.yaml” and to “node_config.yaml” which are both in the NAV2 package 


```
source_ros2
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_pkg_locations_ucsd.yaml

gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml

```



5. Once added, make sure that the **“counter_package_launch_file.launch.py”** file is set to “1” in the “node_config.yaml” to make sure it's activated as well as any other nodes that are desired to be run.
6. Rebuild the workspace


```
build_ros2

```



7. Now launch!


```
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py

```



8. Verify the node is running (which is called **“counter_publisher”)** and echo the topic (which is called **“/counter”)**


```
ros2 node list
ros2 topic echo /counter
```


That's it! A new package has just been integrated into the framework and now can be easily called with any of the framework's launch files.


### 10.2 Integrating supporting files

Supporting files can range from yaml files, data sets, machine learning models and general source code that has nothing to do with ROS but may be required for the node to run properly. Once these files are integrated into the ROS framework, they are used the same exact way as they would be when ROS was not being used, which basically means we need to tell ROS where to locate these files so it can access them. Below is a simple example of a ROS package structure.

ros2_ws

~~	~~src

~~		~~example_package_name

~~			~~config

~~			~~launch

~~			~~example_package_name

~~				~~example_node.py

~~			~~setup.py

Now let's say our node **“example_node.py” **requires an external class or method from a pure python file called **“python_only.py”**, Lets create a new directory or submodule in **“example_package_name”** and call it **“example_submodule_name” **and then put the pure python file there

ros2_ws

~~	~~src

~~		~~example_package_name

~~			~~config

~~			~~launch

~~			~~example_package_name

~~				~~example_submodule_name

~~					~~python_only.py

~~				~~example_node.py

~~			~~setup.py

This is the general idea however the submodule placement is arbitrary as long as you are consistent in the code where things are located. For example, maybe the node requires a pre-trained machine learning model for it to run successfully and makes more sense to have a models folder adjacent to the launch and config directories as shown below

ros2_ws

~~	~~src

~~		~~example_package_name

~~			~~config

~~			~~launch

~~			~~models

~~				~~example_model.pt

~~			~~example_package_name

~~				~~example_node.py

~~			~~setup.py

Again, this placement is arbitrary but it's good to form a convention so others can understand more easily. After the new external files have been added to the package, both the **“setup.py” **and **“example_node.py”** files need to be updated/modified so they can access the supporting files. [See this example of modifying these files in the ROS2 guidebook.](https://docs.google.com/document/d/1DJgVLnu_vN-IXKD3QrQVF3W-JC6RiQPVugHeFAioB58/edit#heading=h.gt05lbgt1rp9)


### 10.3 Integrating new algorithms into the basics package

The 

<p id="gdcalert23" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "basics package"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert24">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[basics package](#heading=h.tlvigrv5i0a6) was created to give a jump start on accessing sensor data and controlling the actuators on the robot without having to focus too much on the ROS implementation. The pre-created nodes have all the ROS-functionality completed and only require the algorithms to process the sensor data and/or control signals for moving the robot. Each node in the package (nodes described in the readme.md link above) has a callback function which provides the starting point for the user to implement their algorithms with ease.




## 11. Navigation

This chapter is dedicated to the various methods for the robot to navigate autonomously.


### 11.1 Lane Detection 

Goal: Be able to identify road lines with opencv and ROS to be able to autonomously navigate around any given track. 

To achieve this, the hardware on the robot must be calibrated for the track environment which is explained in detail below. Once the calibration is complete, launch the robot in an autonomous state and tune the calibration parameters as needed.


#### 11.1.1 Calibration Process

This section is a guide for calibrating the camera to detect road lines as well as for steering and speed control.

While inside docker container, run the calibration script per the instructions found at 

UCSD Robocar ROS Image: [ucsd_robocar_nav1_pkg](https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg#work-flow-to-use-this-repository) (ROS1) or [ucsd_robocar_nav2_pkg](https://gitlab.com/ucsd_robocar2/ucsd_robocar_nav2_pkg#work-flow-to-use-this-repository) (ROS2)


##### 11.1.1.1 ROS1 

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**

From the terminal


```
roslaunch ucsd_robocar_nav1_pkg camera_nav_calibration_launch.launch
```



##### 11.1.1.2 ROS2

**Place the robot on the class provided stand. The wheels of the robot should be clear to spin.**

From the terminal

Modify the hardware config file to turn on the **vesc_without_odom **and the **camera **you have equipped


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate only **all_components **and **camera_nav_calibration **launch files


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Then rebuild and launch 


```
build_ros2
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```



#### 11.1.2 Color Calibration


<table>
  <tr>
   <td>The camera is seeing something like this before we filter
<p>
Anything that is <strong>accepted (true) </strong>will be passed through as <strong>white</strong>, everything else will be<strong> black (false)</strong>. With the default values, everything is white
<p>
We are going to calibrate the camera to only keep the color yellow (the dots in the middle of the road). To do that we need to understand how HSV colors work
   </td>
   <td><p style="text-align: right">


<p id="gdcalert24" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image10.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert25">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image10.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
</table>



<table>
  <tr>
   <td>

<p id="gdcalert25" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image11.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert26">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image11.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>The Hue is often referred to as a degree and goes between 0 and 180
   </td>
  </tr>
</table>



<table>
  <tr>
   <td>So if we chose an H value of 0 we would get something like this
<p>
All of these values are represented by the H value of 0. We would expect something similar to the above square except yellow if we put in the value of 30
   </td>
   <td><p style="text-align: right">


<p id="gdcalert26" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image12.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert27">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image12.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
  <tr>
   <td>Because we are trying to filter out all images except yellow in our image, we will set the lowH and highH to 25 and 35 Here is what we see with these values
<p>
the camera is seeing something like this before we filter
   </td>
   <td><p style="text-align: right">


<p id="gdcalert27" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image13.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert28">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image13.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
</table>



<table>
  <tr>
   <td>We want to keep everything in the upper right corner of these(picture this red square is yellow). As you can see, if the S value gets too low(left to right), we only see white, and if the V value gets too low(up and down) we only see black values. There is not a problem with V or S being too high, as that gives us a pure color(top right). With this in mind, we will keep only what is in the upper right corner by leaving the highS and highV at the max and setting the low values to about half way
   </td>
   <td><p style="text-align: right">


<p id="gdcalert28" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image14.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert29">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image14.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
  <tr>
   <td>Oops, looks like that filters out too much. We can adjust these values until we get what we want. (also notice that in the original image the yellow looked pretty white so we will lower the lowS until we see some good results with little noise)
   </td>
   <td><p style="text-align: right">


<p id="gdcalert29" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image15.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert30">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image15.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
  <tr>
   <td>And there you go (while playing with these two bars (lowS and lowV) will almost always result in a good image like this, note that paying attention to the lighter and darker parts of the color you are filtering for will help you set your lowS and lowV values with a higher accuracy and speed)(if the bright spots are disappearing, allow more S, if the shadows aren't showing up, allow more V)
   </td>
   <td><p style="text-align: right">


<p id="gdcalert30" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image16.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert31">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image16.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
  <tr>
   <td><strong>Inverted_filter </strong>Whatever color you selected to “track” this slider will invert it which is basically rejecting what you were originally tracking and now tracking the exact opposite. This feature is nice when the road color is very consistent and you want to track all road markers, yellow dashed lines, white lanes etc. 
   </td>
   <td>

<p id="gdcalert31" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image17.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert32">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image17.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
</table>



<table>
  <tr>
   <td>If for some reason you still have noise like this and don't want to change these settings, the following settings can be adjusted
   </td>
   <td>

<p id="gdcalert32" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image18.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert33">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image18.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td><strong>gray_thresh </strong>This will put a threshold on what is considered approximately gray such that only the white pixels can pass through the filter and potentially resulting the small noise in the background to black pixels
<p>
<strong>Kernel_size </strong>This value represents the size of the kernel to be convolved with the image with the two transforms, Erosion & Dilation 
<p>
<strong>Erosion_itterations </strong>The higher this value, the more times the kernel is convolved with the image which results in shrinking pixel noise. This means if there is some small noise like in the photo above, the erosion transform will minimize that noise further by shrinking its distribution. The Erosion transform not only minimizes the noise but in general all distributions which means that even our detected road marker will shrink! This issue is fixed with the dilation transform.
<p>
<strong>Dilation_itterations </strong>The higher this value, the more times the kernel is convolved with the image which results in enlarging pixel values. We want to undo the effects of the erosion transform on the road marker which is ok because our noise has already been filtered out so it won't come back!
   </td>
   <td>

<p id="gdcalert33" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image19.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert34">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image19.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
</table>



### 


#### 11.1.3 Line/Lane Calibration 

This part of the calibration is now about manipulating the image dimensions, the geometry of the road lines and parameters to adjust the steering behavior of the robot.


<table>
  <tr>
   <td><strong>min_width </strong>and <strong>max_width </strong>filter based on the size of the dotted lines found 
   </td>
   <td><p style="text-align: right">


<p id="gdcalert34" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image20.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert35">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image20.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
  <tr>
   <td>For example we can see if we set the min to be 15 it will eliminate some of the smaller lines in the distance because they are too thin (below15)
   </td>
   <td><p style="text-align: right">


<p id="gdcalert35" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image21.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert36">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image21.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
</table>



<table>
  <tr>
   <td><strong>Number_of_lines </strong>correlates to the number of lines found in the road to use during calculation. 
<p>
When it is set to 4, only up to 4 lines will be used
   </td>
   <td>

<p id="gdcalert36" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image22.gif). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert37">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image22.gif" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
</table>



<table>
  <tr>
   <td><strong>Error_threshold </strong>Specify the acceptable error the robot will consider as approximately "no error". This means that everything inside the <strong>two vertical red bars (<em><span style="text-decoration:underline;">error bounds)</span></em></strong> will be ignored and only the detected road lines outside of the error bounds are used when determining the steering angle. The simple error cost function implemented will determine the minimum error detected (closets road line) and steer towards it. Again, in this cost function, the road lines within the error bars are ignored. 
<p>
This value also plays a role in determining throttle values which is discussed in the 

<p id="gdcalert37" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "actuator calibration"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert38">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

<a href="#heading=h.hklk77k10dvo">actuator calibration</a> section. 
<p>
Some intuition, if on a <strong>straight path</strong>, 
<p>
If all the detected lines fall within the error bounds, then the algorithm will assume an error of zero and not change its steering angle. So, if the error bars are too wide, this can cause some “drifting” (not like Tokyo drift..) behavior to occur which can make the car go unstable and lose the path. 
   </td>
   <td>

<p id="gdcalert38" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image23.gif). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert39">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image23.gif" width="" alt="alt_text" title="image_tooltip">

<p>
Some intuition, if on a <strong>curved path</strong>, 
<p>
As the distance <strong>decreases</strong> between the error bounds, the robot will steer towards the roadlines that are <strong>closest </strong>to it (basically looking down). 
<p>
As the distance <strong>increases</strong>, the robot will start steering towards detected road lines that are <strong>further </strong>away (will start looking ahead rather than previously).
   </td>
  </tr>
</table>



<table>
  <tr>
   <td><strong>Camera_centerline </strong>is used to calibrate the actual center position of the camera frame. Fastest way to calibrate this is to grab a ruler and align it along the center of the car and then toggle the slider bar such that the <strong>green vertical bar (true car center line)</strong> is lined up directly in the middle of the ruler. The value of slider represents pixel %<strong> </strong>
   </td>
   <td>

<p id="gdcalert39" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image24.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert40">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image24.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td><strong>Frame_width </strong>and <strong>rows_to_watch </strong>are used to crop the image vertically and horizontally
   </td>
   <td>

<p id="gdcalert40" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image25.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert41">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image25.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
</table>



<table>
  <tr>
   <td>And <strong>rows_offset </strong>will give a vertical pan adjustment
   </td>
   <td>

<p id="gdcalert41" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image26.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert42">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image26.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>The higher rows_ofset is, the further down it looks
   </td>
   <td>

<p id="gdcalert42" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image27.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert43">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image27.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
</table>



#### 11.1.4 Actuator Calibration

This part of the calibration is now about identifying the speeds the robot should go in different situations and adjusting how much the car steers left and right.


<table>
  <tr>
   <td>A PID controller is implemented on the steering values to improve performance in autonomous mode.
<p>
<strong>Kp_steering </strong>is the value for the proportional error term
<p>
<strong>Ki_steering </strong>is the value for the integral error term
<p>
<strong>Kd_steering </strong>is the value for the derivative error term
<p>
<strong>Steering_mode</strong>
<ul>

<li><strong>Mode_0</strong> sets  <strong>max left </strong>for fixing any offset

<li><strong>Mode_1</strong> sets <strong>straight</strong>  steering limit

<li><strong>Mode_2</strong>   sets <strong>max right </strong>steering limit

<p>
<strong>Steering_value</strong> is the value used to visualize the steering sensitivity and setting steering constraints.
<p>
<strong>Throttle Calibration</strong> In order to calibrate the throttle you will want to set 3 separate values. See the systems response plot at the end of this section to get more intuition on how throttle scheduling works
<p>
<strong>Throttle _mode</strong>
<ul>

<li><strong>Mode_0</strong> sets <strong>zero_throttle </strong>(<strong>desired throttle for neutral</strong>)

<li><strong>Mode_1</strong> sets <strong>max_throttle</strong> (<strong>desired throttle when there are no errors for road line tracking</strong>)

<li><strong>Mode_2</strong> sets <strong>min_throttle</strong> (<strong>desired throttle with errors present for road line tracking, <span style="text-decoration:underline;">NOT REVERSE</span></strong>)

<p>
<strong>Max RPM </strong>for those using a vesc, this sets upper limit on RPM
<p>
<strong>Steering _polarity</strong> swaps the steering direction
<p>
<strong>Throttle _polarity</strong> swaps the throttle direction
<p>
<strong>Test_motor_control</strong> tests out the PID steering controller and the throttle scheduling values as the car will start tracking the road lines. <span style="text-decoration:underline;">This test should be done on the test stand and not for actual autonomous navigation, that's the next section!</span>
</li>
</ul>
</li>
</ul>
   </td>
   <td><p style="text-align: right">


<p id="gdcalert43" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image28.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert44">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image28.png" width="" alt="alt_text" title="image_tooltip">
</p>

   </td>
  </tr>
</table>



##### 11.1.4.1 Clarification on throttle modes

Again we are going to be setting 3 separate values. The default mode to calibrate first is throttle **mode_0**. When toggling the **throttle_mode **slider to different modes, whatever value that is currently set for the **throttle_value **slider will be the value that is taken in for that mode as the values are being saved to the calibration file in real time. **As long as you are on any particular mode, you are editing the values for that mode, whatever the last value you had when you were on that mode will be the value that is saved to that mode when you end the calibration script.** For example, if the **throttle_value **slider is currently set to 1000 now when toggling the **throttle_mode **slider from **mode_0** to **mode_1, **the value for **mode_0** will remain at 1000 and then the process of editing **mode_1 **will begin. 



<p id="gdcalert44" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image29.jpg). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert45">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image29.jpg "image_tooltip")


In the system response plot, it shows the relationship between the max throttle, min throttle and error threshold values. This is the idea of throttle scheduling based on the tracking error (x-axis). For example, let's say the tracking error is 0.4, based on the throttle plot, the throttle command will be about 0.37 and if the error is less than the error threshold (0.2 in the example throttle plot) then the throttle response will be max throttle which is 0.4 in this example.

From here, exit the calibration script with **ctrl+c** and the chosen throttle mode (mode_0, mode_1 and mode_2) values (and all other calibration values) will be properly stored in the **calibration file found in the config directory of the lane_detection_pkg. <span style="text-decoration:underline;">Don't forget that you need to recompile after calibration.</span>**


#### 11.1.5 Camera Navigation

Only proceed with this section **AFTER **you have gone through the calibration procedure above. At this point, your robot should be taken off of the test stand and put on to the track so it can move freely. Please be alert of the people around you and be ready to shutdown the robot if it starts drifting off the path.


##### 11.1.5.1 ROS1 

From the terminal


```
roslaunch ucsd_robocar_nav1_pkg camera_nav_launch.launch
```



##### 11.1.5.2 ROS2

**Remember, if you make even a single change ANYWHERE in your code (which also includes .yaml files) you must rebuild the package. Check the 

<p id="gdcalert45" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "Source ROS2"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert46">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[Source ROS2](#heading=h.hmc07r1l07ue)** **section.**

From the terminal


```
source_ros2
```


Modify the hardware config file to turn on the **vesc_without_odom **and the **camera **you have equipped


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate only **all_components **and **camera_nav **launch files


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Then rebuild and launch 


```
build_ros2
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```


If the robot is not responding the way you were expecting, turn on the debugger plots which will show you:



* your black and white filter (will show how good your filtering is working)
* the detected lines with bounding boxes and error bound etc. like from **

<p id="gdcalert46" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "calibration"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert47">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[calibration](#heading=h.jd5irxt7cia5)**

This is done easily by setting the ros parameter from the terminal. 



* 1:debug on
* 0:debug off

By default, the debugger is set to 0 (off) for performance reasons and is only recommended to turn on when trying to find out why the robot starts deviating from the expected outcome of following the track. Which is most likely due to changes in environment lighting.

From _<span style="text-decoration:underline;">another</span>_ terminal (turn **on **debugger)


```
ros2 param set /lane_detection_node debug_cv 1
```


From _<span style="text-decoration:underline;">another</span>_ terminal (turn **off **debugger)


```
ros2 param set /lane_detection_node debug_cv 0
```



### 11.2 Tube/Wall Following (coming soon)


### 11.3 SLAM

Simultaneous Localization and Mapping (SLAM) has been completely integrated with our Docker image but is only currently available in ROS1 and **NOT **ROS2. Below is a short tutorial of getting SLAM working on the robot using ROS-Bridge.


#### 11.3.1 Requirements 

Make sure that the following hardware is plugged in and operational before launching the docker container



* Lidar
* Logitech controller (for manual control while mapping)
* VESC or Adafruit PWM board


#### 11.3.2 Starting SLAM

We will need 3 terminals to get SLAM working, 1 for the [Hector-SLAM algorithm in ROS1](http://wiki.ros.org/hector_slam), another for ROS-Bridge and the last one for sensors/hardware and control/path planning algorithms.

From terminal 1


```
source_ros2
```


Modify the hardware config file to turn on the **vesc_with_odom **and the **lidar **you have equipped


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate only **all_components,** **sensor_visualization and f1tenth_vesc_joy_launch **launch files


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Then rebuild


```
build_ros2
```


From terminal 1


```
source_ros1
roslaunch ucsd_robocar_nav1_pkg ros_racer_mapping_launch.launch
```


From terminal 2


```
source_ros_bridge
```


From terminal 3


```
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```


Notice RVIZ is launched automatically with a pre-configured setup file to show a URDF of your robot doing SLAM!

Now depending on what the robot is trying to achieve with slam, modify the all_nodes.yaml file to turn on which navigation/control algorithms for the robot to use. If unsure, or specifically trying to create a map it's suggested to turn on all_components (

<p id="gdcalert47" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "where a lidar and actuator type has been selected"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert48">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[where a lidar and actuator type has been selected](#heading=h.b1yz3c58rzq6)), manual_joy_control_launch to have manual control of the robot while creating the map.


##### 11.3.2.1 Saving the map

There are a few options to do this step. The first option is from the map_server node and the other is from the hector_mapping node. Each provides different output map formats so it could be useful knowing both commands depending on what projects you’ll be working on.


###### 11.3.2.1.1 map_server

For this method, the map files are created in your current working directory so keep that in mind. There is a maps folder in the ucsd_robocar_nav1_pkg that can be used to store all your maps.

From another terminal


```
source_ros1
rosrun map_server map_saver -f ms_map_test
```



###### 11.3.2.1.2 hector_mapping

For this method, the maps generated are saved automatically to the maps directory in ucsd_robocar_nav1_pkg with a generic name with some time stamp. 

From another terminal


```
source_ros1
rostopic pub syscommand std_msgs/String "savegeotiff"
```



#### 11.3.3 Localization in a pre-made map 

This will only load maps that were created with the map_server node! You will also need to modify the car_type in this launch file just as done previously.

From terminal


```
source_ros1
roslaunch ucsd_robocar_nav1_pkg ros_racer_nav_launch.launch
```


Notice RVIZ is launched automatically with a pre-configured setup file to show a URDF of your robot, your saved map and it localizing itself!


## 12. Data Collection

To collect data being broadcasted over the topics that are actively being published,  turn on whichever nodes needed to publish that topic information but make sure that the **rosbag_launch** option in the 

<p id="gdcalert48" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: undefined internal link (link text: "node_config"). Did you generate a TOC with blue links? </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert49">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>

[node_config](#heading=h.vneeh6hyq5n5) is also turned on which is the switch for data collection. This will record ALL topics to the “rosbag” which is a unique file type to ROS. Then a package called [bagpy](https://jmscslgroup.github.io/bagpy/index.html) is used to convert the data into csv format which is useful for viewing/analysis.

Modify the hardware config file to turn on any sensors you have equipped and need for data collection/moving 


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate only **all_components, rosbag_launch **launch files and any other launch file you need to move the robot around** (i.e. manual control, camera_nav etc.)**


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Then rebuild and launch 


```
build_ros2
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```



## 


## 13. F1 Tenth Simulator

A light-weight ROS2 simulator using RVIZ can be used for various scenarios such as model validation, experiment repeatability and general experimentation. The simulator uses a 2D dynamic bicycle-car model to simulate how the car would actually move in an environment. There are several maps that are already made and can be used in the simulator or you can create your own map with the SLAM techniques discussed above and load that map into the simulator as well. Below are the steps to pick the following plug-ins for the simulator: a map, path planning technique, and a controller as an example. Feel free to change any of the plug-ins.

**NOTE**: For the example below, we are going to use the joystick for the controller so you will need a controller plugged into your computer. Since we will be doing manual control, we do not need a path planner activated.

**NOTE: Only use the simulator on the <span style="text-decoration:underline;">X86 docker image</span> and not the Jetson.**

Modify the hardware config file to turn off any sensors you have


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/car_config.yaml
```


Then modify the node config file to activate only the **simulator **and **f1tenth_vesc_joy_launch, **launch files and any other launch file you need to move the robot around** (i.e. manual control, camera_nav etc.)**


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/node_config.yaml
```


Modify the f1 tenth simulator config file to update the map (if needed)


```
gedit src/ucsd_robocar_hub2/ucsd_robocar_nav2_pkg/config/f1_tenth_sim.yaml
```


Then rebuild and launch 


```
build_ros2
ros2 launch ucsd_robocar_nav2_pkg all_nodes.launch.py
```



### 13.1 Creating a Map with Paint (coming soon)


### 13.2 Updating Vehicle Parameters (coming soon)


### 13.3 Adding Multiple Vehicles (coming soon)


## 


## 14. Troubleshooting

Below are the links to the troubleshooting sections when using either ROS1 or ROS2. There are troubleshooting guides for every single package to potentially help solve any common problems.



* **[ucsd_robocar_hub1 troubleshooting links](https://gitlab.com/ucsd_robocar/ucsd_robocar_nav1_pkg#troubleshooting)**
* **[ucsd_robocar_hub2 troubleshooting links](https://gitlab.com/ucsd_robocar2/ucsd_robocar_hub2#troubleshooting)**


## 15. Frequently Used Linux commands


### 15.1 WIFI

Rescan wifi list: `sudo nmcli device wifi rescan`

Show wifi list: `sudo nmcli device wifi list`

Connect to wifi network: `sudo nmcli device wifi connect &lt;NETWORK_NAME> password &lt;NETWORK_PASSWORD>`

Restart networking: `sudo service NetworkManager restart ` 

Check network interfaces: `nmcli device status` 

Check if connected internet: `ping google.com`

Disable power save mode for wifi: `sudo iw dev wlan0 set power_save off`

Networking info: `ifconfig `


### 15.2 Hardware Tests

List connected USB devices: `lsusb`

Check if joystick is working: `jstest /dev/input/js0`

Check if x_11 forwarding is working: `xeyes`


### 15.3 File management

Listing files in a directory: `ls`

Copy file: `cp old_file_name new_file_name`

Copy directory: `cp -r old_directory_name new_directory_name`

Move file: `mv file_name /path/to/new/file/location/file_name`

Move directory: `mv -r directory_name /path/to/new/directory/location/directory_name`

Delete file: `rm -f file_name`

Delete directory: `rm -rf directory_name`

To copy a file from B to A while logged into B: 


```
scp /path/to/file username@A_ip_address:/path/to/destination
```


To copy a file from B to A while logged into A:


```
scp username@B_ip_address:/path/to/file /path/to/destination
```



### 15.4 System Control

Terminate process by PID: `sudo kill -9 PID_number`
